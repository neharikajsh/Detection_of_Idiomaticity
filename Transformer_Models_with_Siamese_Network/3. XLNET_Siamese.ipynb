{"cells":[{"cell_type":"markdown","source":["# **Setup**"],"metadata":{"id":"-bqHml7Do4Cr"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8gXdCtnjq1PK","outputId":"4352d26c-2472-46ed-bf42-9e9197bbaaf1","executionInfo":{"status":"ok","timestamp":1673614702966,"user_tz":0,"elapsed":283,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Idiomacity-Detection' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/niyaryca/Idiomacity-Detection.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1OczNFNq4_7","outputId":"fbaad59c-8568-46e0-ade1-90344f89aa0d","executionInfo":{"status":"ok","timestamp":1673614706875,"user_tz":0,"elapsed":3613,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","/content\n"]}],"source":["!pip install transformers\n","%cd /content/ "]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzqAwaLCm09N","executionInfo":{"status":"ok","timestamp":1673614710593,"user_tz":0,"elapsed":3732,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}},"outputId":"91664cf8-1bbf-431b-990b-46436a385d2c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.8.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jj_uaj3aq8Od","executionInfo":{"status":"ok","timestamp":1673614710594,"user_tz":0,"elapsed":10,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["import site\n","site.main()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gSUai3g0q-Ji","executionInfo":{"status":"ok","timestamp":1673614710594,"user_tz":0,"elapsed":9,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["import os\n","import csv\n","\n","from pathlib import Path"]},{"cell_type":"markdown","source":["# **Preprocessing Data**"],"metadata":{"id":"i08_Ltd6o0eM"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"jst76TcOq_kX","executionInfo":{"status":"ok","timestamp":1673614710595,"user_tz":0,"elapsed":9,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["def load_csv( path, delimiter=',' ) : \n","  header = None\n","  data   = list()\n","  with open( path, encoding='utf-8') as csvfile:\n","    reader = csv.reader( csvfile, delimiter=delimiter ) \n","    for row in reader : \n","      if header is None : \n","        header = row\n","        continue\n","      data.append( row ) \n","  return header, data"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"P6SA-GvKrBSY","executionInfo":{"status":"ok","timestamp":1673614710595,"user_tz":0,"elapsed":9,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["def write_csv( data, location ) : \n","  with open( location, 'w', encoding='utf-8') as csvfile:\n","    writer = csv.writer( csvfile ) \n","    writer.writerows( data ) \n","  print( \"Wrote {}\".format( location ) ) \n","  return"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KHJHosMVVNXS","executionInfo":{"status":"ok","timestamp":1673614710595,"user_tz":0,"elapsed":8,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["class Node():\n","  def __init__(self, sentence, label):\n","    self.sentence = sentence\n","    self.label = label\n","\n","def create_idiom_dict_train(data_location, file_name) :\n","    idiom_dict = {}\n","    file_name = os.path.join( data_location, file_name ) \n","    header, data = load_csv( file_name )\n","    for elem in data:\n","        label     = elem[ header.index( 'Label'  ) ]\n","        sentence = elem[ header.index( 'Target' ) ]\n","        idiom = elem[ header.index( 'MWE' ) ]\n","        if idiom in idiom_dict:\n","          idiom_dict[idiom].append(Node(sentence, label))\n","        else:\n","          idiom_dict[idiom] = [Node(sentence, label)]\n","    return idiom_dict"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xWO_TnshqY21","executionInfo":{"status":"ok","timestamp":1673614710595,"user_tz":0,"elapsed":8,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["d1 = create_idiom_dict_train('/content/Idiomacity-Detection/Rawdata', 'train_zero_shot.csv')\n","d2 = create_idiom_dict_train('/content/Idiomacity-Detection/Rawdata', 'train_one_shot.csv')\n","for key, value in d2.items():\n","  if key in d1:\n","    d1[key].append(value)\n","  else:\n","    d1[key] = value"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"JvkkIptrrDsB","executionInfo":{"status":"ok","timestamp":1673614710865,"user_tz":0,"elapsed":277,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["def _get_train_data( data_location, file_name) :\n","    \n","    file_name = os.path.join( data_location, file_name ) \n","\n","    header, data = load_csv( file_name )\n","\n","    out_header = [ 'label1', 'label2', 'sentence1', 'sentence2', 'sentence3', 'sentence4' ]\n","\n","    out_data = list()\n","    for elem1 in data :\n","        label     = elem1[ header.index( 'Label'  ) ]\n","        sentence1 = elem1[ header.index( 'Target' ) ]\n","        for elem2 in d1[elem1[ header.index( 'MWE' ) ]]:\n","          if elem2.sentence != sentence1:\n","              label2     = elem2.label\n","              sentence2 = elem2.sentence\n","              this_row = None\n","              sentence3 = elem1[ header.index( 'MWE' ) ]\n","              sentence4 = sentence3\n","              this_row = [ label, label2, sentence1, sentence3, sentence2, sentence4]\n","              out_data.append( this_row )\n","              assert len( out_header ) == len( this_row )\n","    return [ out_header ] + out_data"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"rMWvYWX8rGsu","executionInfo":{"status":"ok","timestamp":1673614710865,"user_tz":0,"elapsed":8,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_idiom ) :\n","\n","    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n","    gold_header  = gold_data = None\n","    if not gold_file_name is None : \n","        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n","        assert len( input_data ) == len( gold_data )\n","\n","    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n","    # ['ID', 'DataID', 'Language', 'Label']\n","    \n","    out_header = [ 'label1', 'label2', 'sentence1', 'sentence3' ]\n","    if include_idiom :\n","        out_header = [ 'label1', 'label2', 'label3', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'sentence6', 'language' ]\n","\n","    out_data = list()\n","    for index in range( len( input_data ) ) :\n","        label = 1\n","        if not gold_file_name is None : \n","            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n","            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n","            assert this_input_id == this_gold_id\n","            \n","            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n","            language = gold_data[index][gold_header.index('Language')]\n","        elem      = input_data[ index ]\n","        sentence1 = elem[ input_headers.index( 'Target' ) ]\n","        this_row = None\n","        if not include_idiom :\n","            this_row = [ label, sentence1 ] \n","        else :\n","            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n","            this_row = [ label, sentence1, sentence2 ]\n","        idiom = elem[ input_headers.index( 'MWE' ) ]\n","        other_nodes = d1[idiom]\n","        if(len(other_nodes)==1):\n","            if not include_idiom :\n","                this_row = [ label, other_nodes[0].label, sentence1, other_nodes[0].sentence ] \n","            else :\n","                sentence2 = elem[ input_headers.index( 'MWE' ) ]\n","                this_row = [ label, other_nodes[0].label, other_nodes[0].label, sentence1, sentence2, other_nodes[0].sentence, sentence2, other_nodes[0].sentence, sentence2, language ]\n","        else:\n","            if not include_idiom :\n","                this_row = [ label, other_nodes[0].label, sentence1, other_nodes[0].sentence ] \n","            else :\n","                sentence2 = elem[ input_headers.index( 'MWE' ) ]\n","                this_row = [ label, other_nodes[0].label, other_nodes[1].label, sentence1, sentence2, other_nodes[0].sentence, sentence2, other_nodes[1].sentence, sentence2, language ]\n","           \n","        assert len( out_header ) == len( this_row ) \n","        out_data.append( this_row )\n","        \n","\n","    return [ out_header ] + out_data"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"7t8zqdt9zAQo","executionInfo":{"status":"ok","timestamp":1673614710866,"user_tz":0,"elapsed":7,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["train_zero_data = _get_train_data(\n","        data_location   = '/content/Idiomacity-Detection/Rawdata',\n","        file_name       = 'train_zero_shot.csv'\n","    )\n","train_one_data = _get_train_data(\n","        data_location   = '/content/Idiomacity-Detection/Rawdata',\n","        file_name       = 'train_one_shot.csv'\n","    )\n","\n","assert train_zero_data[0] == train_one_data[0] ## Headers\n","train_data = train_one_data + train_zero_data[1:]\n","\n","dev_data = _get_dev_eval_data(\n","        data_location    = '/content/Idiomacity-Detection/Rawdata',\n","        input_file_name  = 'dev.csv',\n","        gold_file_name   = 'dev_gold.csv',\n","        include_idiom    = True\n","    )"]},{"cell_type":"markdown","source":["# **Importing Libraries**"],"metadata":{"id":"0p4PiqU0otBu"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"183UBr4ArRSh","executionInfo":{"status":"ok","timestamp":1673614715280,"user_tz":0,"elapsed":4420,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["import os\n","import sys\n","import numpy as np\n","import random\n","import pickle\n","import logging\n","\n","from typing          import Optional\n","from dataclasses     import dataclass, field\n","from sklearn.metrics import f1_score, accuracy_score\n","\n","from datasets        import load_dataset, load_metric\n","\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoModel,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    PretrainedConfig,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.utils         import check_min_version\n","from transformers.trainer_utils import get_last_checkpoint, is_main_process\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","from keras_preprocessing.sequence import pad_sequences\n","import torch"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"qtZiIQ5SrXBd","executionInfo":{"status":"ok","timestamp":1673614715281,"user_tz":0,"elapsed":27,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n","check_min_version(\"4.6.0.dev0\")\n","\n","task_to_keys = {\n","    \"cola\": (\"sentence\", None),\n","    \"mnli\": (\"premise\", \"hypothesis\"),\n","    \"mrpc\": (\"sentence1\", \"sentence2\"),\n","    \"qnli\": (\"question\", \"sentence\"),\n","    \"qqp\": (\"question1\", \"question2\"),\n","    \"rte\": (\"sentence1\", \"sentence2\"),\n","    \"sst2\": (\"sentence\", None),\n","    \"stsb\": (\"sentence1\", \"sentence2\"),\n","    \"wnli\": (\"sentence1\", \"sentence2\"),\n","}\n","\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0EdhT3XrZCu","outputId":"942012e8-2f09-4ff3-cb3b-c6c773b4f43e","executionInfo":{"status":"ok","timestamp":1673614715282,"user_tz":0,"elapsed":25,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['label1', 'label2', 'sentence1', 'sentence2', 'sentence3', 'sentence4'],\n"," ['0',\n","  '1',\n","  'Program leaders said the scholarship defines public service broadly and imagines a variety of pathways toward civic engagement.',\n","  'public service',\n","  'In the ensuing years, Wennberg might not have managed to knock down the parking deck, but his administration helped keep Central Vermont Public Service from moving its corporate headquarters out of the city, and after successfully fighting a number of shopping centers city officials worried would pose a threat to downtown, he negotiated a deal that kept Diamond Run Mall from hosting a movie theater or supermarket and got the city a couple million dollars in payments that funded a variety of projects through the years.',\n","  'public service'],\n"," ['1',\n","  '0',\n","  'In the ensuing years, Wennberg might not have managed to knock down the parking deck, but his administration helped keep Central Vermont Public Service from moving its corporate headquarters out of the city, and after successfully fighting a number of shopping centers city officials worried would pose a threat to downtown, he negotiated a deal that kept Diamond Run Mall from hosting a movie theater or supermarket and got the city a couple million dollars in payments that funded a variety of projects through the years.',\n","  'public service',\n","  'Program leaders said the scholarship defines public service broadly and imagines a variety of pathways toward civic engagement.',\n","  'public service'],\n"," ['0',\n","  '1',\n","  'Blockchains, fundamentally, are banking because what they’re doing is allowing the transaction of value across networks … they’re doing it in an orthogonally different way,\" he said Wednesday in what may be his swan song in public office.',\n","  'swan song',\n","  'They’ve also got:“ Killers of the Flower Moon,” directed by Martin Scorsese and starring Leonardo DiCaprio and Robert De Niro; “Swan Song” with Mahershala Ali, Naomie Harris and Glenn Close and Awkwafina; “Kitbag,” from Ridley Scott and Joaquin Phoenix; A24’s “Sharper” with Julianne Moore; and “Snow Blind” with Jake Gyllenhaal.',\n","  'swan song']]"]},"metadata":{},"execution_count":15}],"source":["train_data[0:4]"]},{"cell_type":"code","source":["dev_data[0:4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmvwVfzQd7sZ","executionInfo":{"status":"ok","timestamp":1673614715282,"user_tz":0,"elapsed":21,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}},"outputId":"cc8b42d5-dcd0-44dc-cbf3-8f02757bf100"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['label1',\n","  'label2',\n","  'label3',\n","  'sentence1',\n","  'sentence2',\n","  'sentence3',\n","  'sentence4',\n","  'sentence5',\n","  'sentence6',\n","  'language'],\n"," ['1',\n","  '1',\n","  '1',\n","  'Are these interruptions of the good life a necessary condition of the high life?',\n","  'high life',\n","  'Despite having the riches to afford the high life, PSG captain Marquinhos is still in touch with his past life before becoming a multi-millionaire footballer.',\n","  'high life',\n","  'Despite having the riches to afford the high life, PSG captain Marquinhos is still in touch with his past life before becoming a multi-millionaire footballer.',\n","  'high life',\n","  'EN'],\n"," ['1',\n","  '1',\n","  '1',\n","  \"But for Australian fashion designer Abby Kheir, there's no reason not to treat her employees to a taste of the high life all-year round.\",\n","  'high life',\n","  'Despite having the riches to afford the high life, PSG captain Marquinhos is still in touch with his past life before becoming a multi-millionaire footballer.',\n","  'high life',\n","  'Despite having the riches to afford the high life, PSG captain Marquinhos is still in touch with his past life before becoming a multi-millionaire footballer.',\n","  'high life',\n","  'EN'],\n"," ['1',\n","  '1',\n","  '1',\n","  'With that, I will be enjoying the pleasures of the high life knowing I earned money the hard way: Gambling with a bit of mega luck.',\n","  'high life',\n","  'Despite having the riches to afford the high life, PSG captain Marquinhos is still in touch with his past life before becoming a multi-millionaire footballer.',\n","  'high life',\n","  'Despite having the riches to afford the high life, PSG captain Marquinhos is still in touch with his past life before becoming a multi-millionaire footballer.',\n","  'high life',\n","  'EN']]"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# **Tokenizer**"],"metadata":{"id":"CLaJ35Tco-1c"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"0QJsCAgjzt-9","executionInfo":{"status":"ok","timestamp":1673614716430,"user_tz":0,"elapsed":1161,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large',\n","                                          cache_dir=None,\n","                                          use_fast=True,\n","                                          revision=\"main\",\n","                                          use_auth_token=None,)"]},{"cell_type":"code","source":["def shuffle_data(data):\n","    indices = list(range(len(data)))\n","    random.shuffle(indices)\n","    shuffled_data = []\n","    for i in indices:\n","        shuffled_data.append(data[i])\n","    return shuffled_data"],"metadata":{"id":"rxajbXyYtpJp","executionInfo":{"status":"ok","timestamp":1673614716432,"user_tz":0,"elapsed":21,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"nUGKP2VK0PY6","executionInfo":{"status":"ok","timestamp":1673614716433,"user_tz":0,"elapsed":19,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","def Preprocess_Data(input, tokenizer, max_len, batch_size, data_class=\"train\"):\n","    input1 = []\n","    input2 = []\n","    label1 = []\n","    label2 = []\n","    for i in input:\n","      \"\"\"if(i[1]!='1' and i[1]!='0'):\n","        continue\"\"\"\n","      label1.append(int(i[0]))\n","      label2.append(int(i[1]))\n","      args = (\n","            (i[2], i[3])\n","      )\n","      input1.append(args)\n","      args = (\n","            (i[4], i[5])\n","      )\n","      input2.append(args)\n","    encoded_input1 = tokenizer(input1, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n","    encoded_input2 = tokenizer(input2, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n","    \n","    input_ids1 = encoded_input1['input_ids']\n","    attention_mask1 = encoded_input1['attention_mask']\n","    labels1 = torch.tensor(label1)\n","\n","    input_ids2 = encoded_input2['input_ids']\n","    attention_mask2 = encoded_input2['attention_mask']\n","    labels2 = torch.tensor(label2)\n","\n","    dataset_tensor = TensorDataset(input_ids1, attention_mask1, labels1, input_ids2, attention_mask2, labels2)\n","\n","    if data_class == \"train\":\n","        sampler = RandomSampler(dataset_tensor)\n","    else:\n","        sampler = SequentialSampler(dataset_tensor)\n","    dataloader = DataLoader(dataset_tensor, sampler=sampler, batch_size=batch_size)\n","\n","    return dataloader"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"uIP79MjDtx5r","executionInfo":{"status":"ok","timestamp":1673614716433,"user_tz":0,"elapsed":18,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["max_len = 512\n","batch_size = 32\n","def PreProcess_Dev(input, tokenizer, max_len, batch_size, data_class=\"dev\"):\n","    input1 = []\n","    input2 = []\n","    input3 = []\n","    label1 = []\n","    label2 = []\n","    label3 = []\n","    language = []\n","    for i in input:\n","      \"\"\"if(i[1]!='1' and i[1]!='0'):\n","        continue\"\"\"\n","      label1.append(int(i[0]))\n","      label2.append(int(i[1]))\n","      label3.append(int(i[2]))\n","      args = (\n","            (i[3], i[4])\n","      )\n","      input1.append(args)\n","      args = (\n","            (i[5], i[6])\n","      )\n","      input2.append(args)\n","      args = (\n","          (i[7], i[8])\n","      )\n","      input3.append(args)\n","      if i[9]=='EN':\n","        language.append(0)\n","      else:\n","        language.append(1)\n","    encoded_input1 = tokenizer(input1, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n","    encoded_input2 = tokenizer(input2, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n","    encoded_input3 = tokenizer(input3, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n","    input_ids1 = encoded_input1['input_ids']\n","    attention_mask1 = encoded_input1['attention_mask']\n","    labels1 = torch.tensor(label1)\n","    print(input_ids1.size(), attention_mask1.size(), labels1.size())\n","\n","    input_ids2 = encoded_input2['input_ids']\n","    attention_mask2 = encoded_input2['attention_mask']\n","    labels2 = torch.tensor(label2)\n","\n","    print(input_ids2.size(), attention_mask2.size(), labels2.size())\n","\n","    input_ids3 = encoded_input3['input_ids']\n","    attention_mask3 = encoded_input3['attention_mask']\n","    labels3 = torch.tensor(label3)\n","\n","    print(input_ids3.size(), attention_mask3.size(), labels3.size())  \n","    language = torch.tensor(language)  \n","\n","    dataset_tensor = TensorDataset(input_ids1, attention_mask1, labels1, input_ids2, attention_mask2, labels2, input_ids3, attention_mask3, labels3, language)\n","\n","    if data_class == \"train\":\n","        sampler = RandomSampler(dataset_tensor)\n","    else:\n","        sampler = SequentialSampler(dataset_tensor)\n","    dataloader = DataLoader(dataset_tensor, sampler=sampler, batch_size=batch_size)\n","\n","    return dataloader"]},{"cell_type":"code","source":["import torch.nn as nn\n","class SiameseModel(nn.Module):\n","    def __init__(self):\n","        super(SiameseModel, self).__init__()\n","        \n","        self.base_model = AutoModel.from_pretrained(\n","            'xlm-roberta-base',\n","            from_tf=bool(\".ckpt\" in 'xlm-roberta-base'),\n","            config=config,\n","            cache_dir=None,\n","            revision=\"main\",\n","            use_auth_token=None,\n","        ).cuda()\n","        self.dropout = nn.Dropout(0.5)\n","        self.linear = nn.Linear(768, 2).cuda() \n","        \n","    def forward(self, input_ids1, attn_mask1, input_ids2, attn_mask2):\n","        \n","        outputs1 = self.base_model(input_ids1, attention_mask=attn_mask1).last_hidden_state[:, 0]\n","        outputs2 = self.base_model(input_ids2, attention_mask=attn_mask2).last_hidden_state[:, 0]\n","        difference = outputs1*outputs2\n","        \n","        outputs = self.dropout(difference)\n","        outputs = self.linear(outputs)\n","        \n","        return outputs"],"metadata":{"id":"Az8bL3kvNAST","executionInfo":{"status":"ok","timestamp":1673614716434,"user_tz":0,"elapsed":18,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# import torch.nn as nn\n","#  class SiameseModel(nn.Module):\n","#      def __init__(self):\n","#          super(SiameseModel, self).__init__()\n","        \n","# #         self.base_model = AutoModel.from_pretrained(\n","# #             'xlm-roberta-large',\n","# #             from_tf=bool(\".ckpt\" in 'xlm-roberta-large'),\n","# #             config=config,\n","# #             cache_dir=None,\n","# #             revision=\"main\",\n","# #             use_auth_token=None,\n","# #         ).cuda()\n","# #         self.dropout = nn.Dropout(0.5)\n","# #         self.linear = nn.Linear(768, 2).cuda() \n","        \n","# #     def forward(self, input_ids1, attn_mask1, input_ids2, attn_mask2):\n","        \n","# #         first_output = self.base_model(input_ids1, attention_mask=attn_mask1).last_hidden_state[:, 0]\n","# #         second_output = self.base_model(input_ids2, attention_mask=attn_mask2).last_hidden_state[:, 0]\n","# #         #Similarity score calculated using dot product & the value will be between 1 & 0\n","# #         dot_product = first_output*second_output\n","\n","# #         final_output = self.dropout(dot_product)\n","# #         final_output = self.linear(final_output)\n","        \n","# ###         return final_output"],"metadata":{"id":"gGO8FbtaNI1D","executionInfo":{"status":"ok","timestamp":1673614716435,"user_tz":0,"elapsed":18,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"id":"05DDBBjOYzrs","executionInfo":{"status":"ok","timestamp":1673614716435,"user_tz":0,"elapsed":17,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["from sklearn.metrics import f1_score, accuracy_score\n","def Eval(bert_model, dataloader):\n","\n","    bert_model.eval()\n","    predictions, true_labels = [], []\n","    predictions_en, true_labels_en = [], []\n","    predictions_pt, true_labels_pt = [], []\n","    num_correct = 0\n","    \n","    for step, batch in enumerate(tqdm(dataloader)):\n","        batch = tuple(t.to(device) for t in batch)\n","        with torch.no_grad():\n","            logits1 = nn.functional.softmax(bert_model.forward(batch[0], batch[1], batch[3], batch[4]), -1)\n","        with torch.no_grad():\n","            logits2 = nn.functional.softmax(bert_model.forward(batch[0], batch[1], batch[6], batch[7]), -1)\n","        logits = torch.cat((logits1, logits2), dim=1)\n","        max_args = torch.argmax(logits, dim=1)\n","        batch_predictions = []\n","        batch_true_labels = batch[2]\n","        first_sentence_labels = batch[5]\n","        second_sentence_labels = batch[8]\n","        batch_en_predictions = []\n","        batch_pt_predictions = []\n","        true_en_predictions = []\n","        true_pt_predictions = []\n","        language = batch[9]\n","        for idx, instance in enumerate(max_args):\n","          if instance == 0:\n","            batch_predictions.append((first_sentence_labels[idx] - 1) * -1) # 0, 1 toggle\n","            if language[idx].item() == 0:\n","              batch_en_predictions.append((first_sentence_labels[idx] - 1) * -1)\n","              true_en_predictions.append(batch_true_labels[idx])\n","            else:\n","              batch_pt_predictions.append((first_sentence_labels[idx] - 1) * -1)\n","              true_pt_predictions.append(batch_true_labels[idx])\n","          elif instance == 1:\n","            batch_predictions.append(first_sentence_labels[idx])\n","            if language[idx].item() == 0:\n","              batch_en_predictions.append(first_sentence_labels[idx])\n","              true_en_predictions.append(batch_true_labels[idx])\n","            else:\n","              batch_pt_predictions.append(first_sentence_labels[idx])\n","              true_pt_predictions.append(batch_true_labels[idx])\n","          elif instance == 2:\n","            batch_predictions.append((second_sentence_labels[idx] - 1) * -1)\n","            if language[idx].item() == 0:\n","              batch_en_predictions.append((second_sentence_labels[idx] - 1) * -1)\n","              true_en_predictions.append(batch_true_labels[idx])\n","            else:\n","              batch_pt_predictions.append((second_sentence_labels[idx] - 1) * -1)\n","              true_pt_predictions.append(batch_true_labels[idx])\n","          else:\n","            batch_predictions.append(second_sentence_labels[idx])\n","            if language[idx].item() == 0:\n","              batch_en_predictions.append(second_sentence_labels[idx])\n","              true_en_predictions.append(batch_true_labels[idx])\n","            else:\n","              batch_pt_predictions.append(second_sentence_labels[idx])\n","              true_pt_predictions.append(batch_true_labels[idx])\n","        predictions += batch_predictions\n","        true_labels += batch_true_labels\n","        predictions_en += batch_en_predictions\n","        predictions_pt += batch_pt_predictions\n","        true_labels_en += true_en_predictions\n","        true_labels_pt += true_pt_predictions\n","    return true_labels, predictions, true_labels_en, predictions_en, true_labels_pt, predictions_pt"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"jYDcfd5eY2oR","executionInfo":{"status":"ok","timestamp":1673614716436,"user_tz":0,"elapsed":18,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["def metrics(true_labels, predictions):\n","    pre = []\n","    tl = []\n","    num_correct = 0\n","    for pred, true_label in zip(predictions, true_labels):\n","        pre.append(int(pred.item()))\n","        tl.append(int(true_label.item()))\n","        if pred == true_label:\n","            num_correct += 1\n","    print(\"\\nAccuracy: %s\" % (float(num_correct) / float(len(true_labels))))\n","    print(\"F1 Score \")\n","    print(f1_score(tl, pre, average='macro'))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"np7YcOhL4IWQ","executionInfo":{"status":"ok","timestamp":1673614716437,"user_tz":0,"elapsed":18,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[],"source":["from tqdm import tqdm\n","\n","def Train_Eval(bert_model, train_data, lr, n_epoch, tokenizer, batch_size, max_len):\n","\n","    print(\"Start Training!\")\n","    optimizer = AdamW(bert_model.parameters(), lr=lr)\n","    bert_model.train()\n","    dev_dataloader = PreProcess_Dev(dev_data[1:], tokenizer, max_len, batch_size, data_class=\"dev\")\n","    # TRAIN loop\n","    for epoch in range(n_epoch):\n","        shuffled_train_data = shuffle_data(train_data)\n","        shuffled_train_data = Preprocess_Data(shuffled_train_data, tokenizer, max_len, batch_size)\n","        print(f\"\\nEpoch {epoch}\")\n","        torch.cuda.empty_cache()\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        for step, batch in enumerate(tqdm(shuffled_train_data)):\n","            batch = tuple(t.to(device) for t in batch)\n","            bert_model.zero_grad()\n","            # forward pass\n","            logits = bert_model.forward(batch[0], batch[1], batch[3], batch[4])\n","            # print(loss)\n","            loss = 0\n","            target = torch.where(batch[2]==batch[5], 1, 0)\n","            #target = target.reshape(-1,1)\n","            loss = nn.functional.cross_entropy(logits, target)\n","            \n","            # backward pass\n","            loss.backward()\n","            # track train loss\n","            tr_loss += loss.item()\n","            nb_tr_steps += 1\n","            #loss = loss.detach()\n","            # update parameters\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            \n","\n","        # print train loss per epoch\n","        print(\"Train loss on epoch {}: {}\\n\".format(epoch, tr_loss / nb_tr_steps))\n","\n","    true_labels, predictions, true_labels_en, predictions_en, true_labels_pt, predictions_pt  = Eval(bert_model, dev_dataloader)\n","    print(\"EN-PT\")\n","    metrics(true_labels, predictions)\n","    print(\"EN\")\n","    metrics(true_labels_en, predictions_en)\n","    print(\"PT\")\n","    metrics(true_labels_pt, predictions_pt)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ar64c-jf4rp-","outputId":"47e0d472-b8c3-4e4a-9f8e-880a34a4e8ff","executionInfo":{"status":"error","timestamp":1673614723461,"user_tz":0,"elapsed":7041,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-f4d09d10636d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiameseModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mn_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-39e21a8e4a10>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSiameseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         self.base_model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;34m'xlm-roberta-base'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ckpt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'xlm-roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2377\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2379\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2380\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit)\u001b[0m\n\u001b[1;32m   2693\u001b[0m                     \u001b[0;34m\"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m                 )\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for XLMRobertaModel:\n\tsize mismatch for roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([250002, 768]) from checkpoint, the shape in current model is torch.Size([250002, 1024]).\n\tsize mismatch for roberta.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([514, 1024]).\n\tsize mismatch for roberta.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 1024]).\n\tsize mismatch for roberta.embeddings.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.embeddings.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for roberta.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for roberta.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for roberta.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for roberta.pooler.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for roberta.pooler.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."]}],"source":["config = AutoConfig.from_pretrained(\n","        'xlm-roberta-large',\n","        num_labels=2,\n","        finetuning_task=None,\n","        cache_dir=None,\n","        revision=\"main\",\n","        use_auth_token=None,\n","    )\n","\n","model = SiameseModel()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","\n","learning_rate = 1e-5\n","num_epoch = 3                         \n","torch.cuda.empty_cache()\n","max_len = 128\n","batch_size = 128\n","\n","if n_gpu > 1:\n","    model.to(device)\n","    model = torch.nn.DataParallel(model)\n","else:\n","    model.cuda()\n","\n","Train_Eval(model, train_data[1:], learning_rate, num_epoch, tokenizer, batch_size, max_len)\n"]},{"cell_type":"code","source":["%%shell\n","jupyter nbconvert --to html 2_MBERT_Siamese.ipynb"],"metadata":{"id":"GX_dxcPXEtvC","executionInfo":{"status":"aborted","timestamp":1673614723462,"user_tz":0,"elapsed":8,"user":{"displayName":"Neharika Joshi","userId":"10536822327511302738"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}