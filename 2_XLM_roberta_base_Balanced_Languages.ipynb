{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "do-TXGBemGgH"
      ],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-TXGBemGgH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WsITUAnzvFl"
      },
      "source": [
        "Download the Task data and evaluation scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3qhQdpl-1-",
        "outputId": "64f31bd4-5bbd-42e2-adf7-5440233d02d4"
      },
      "source": [
        "!git clone https://github.com/niyaryca/Idiomacity-Detection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Idiomacity-Detection'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 95 (delta 43), reused 39 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60w-An2vzikk"
      },
      "source": [
        "Download and install an editable version of huggingfaces transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8BhcLYcmVvd",
        "outputId": "3f468300-787b-4330-b507-aeb1df2037aa"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVMnwTSzmjJ"
      },
      "source": [
        "Required for run_glue ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tsWits5tw1t",
        "outputId": "d85ee581-7ef3-4577-92d6-c99aa0764327"
      },
      "source": [
        "## run_glue needs this.\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-igYdTTgzp9e"
      },
      "source": [
        "Editable install requires runtime restart unless we do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOuKplBmmbeB"
      },
      "source": [
        "import site\n",
        "site.main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvC8kAGNnKk_"
      },
      "source": [
        "# Imports and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOw3MaG7nN77"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzDtW9eXnOhG"
      },
      "source": [
        "def load_csv( path, delimiter=',' ) :\n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile, delimiter=delimiter )\n",
        "    for row in reader :\n",
        "      if header is None :\n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row )\n",
        "  return header, data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwtDsdtAnSZu"
      },
      "source": [
        "def write_csv( data, location ) :\n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile )\n",
        "    writer.writerows( data )\n",
        "  print( \"Wrote {}\".format( location ) )\n",
        "  return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Io3D3_z4wt"
      },
      "source": [
        "The following function creates a submission file from the predictions output by run_glue (the text classification script from huggingface transformers - see below).\n",
        "\n",
        "Note that we set it up so we can load up results for only one setting.\n",
        "\n",
        "It requires as input the submission format file, which is available with the data. You can call this after completing each setting to load up results for both settings (see below).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re31vnLoQWww"
      },
      "source": [
        "def insert_to_submission_file( submission_format_file, input_file, prediction_format_file, setting ) :\n",
        "    submission_header, submission_content = load_csv( submission_format_file )\n",
        "    input_header     , input_data         = load_csv( input_file             )\n",
        "    prediction_header, prediction_data    = load_csv( prediction_format_file, '\\t' )\n",
        "\n",
        "    assert len( input_data ) == len( prediction_data )\n",
        "\n",
        "    ## submission_header ['ID', 'Language', 'Setting', 'Label']\n",
        "    ## input_header      ['label', 'sentence1' ]\n",
        "    ## prediction_header ['index', 'prediction']\n",
        "\n",
        "    prediction_data = list( reversed( prediction_data ) )\n",
        "\n",
        "    started_insert  = False\n",
        "    for elem in submission_content :\n",
        "        if elem[ submission_header.index( 'Setting' ) ] != setting :\n",
        "            if started_insert :\n",
        "                if len( prediction_data ) == 0 :\n",
        "                    break\n",
        "                else :\n",
        "                    raise Exception( \"Update should to contiguous ... something wrong.\" )\n",
        "            continue\n",
        "        started_insert = True\n",
        "        elem[ submission_header.index( 'Label' ) ] = prediction_data.pop()[ prediction_header.index( 'prediction' ) ]\n",
        "\n",
        "    return [ submission_header ] + submission_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44LyZ-OXmgQW"
      },
      "source": [
        "# Pre-process: Create train and dev and evaluation data in required format\n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”).\n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-3ymBcEmxaV"
      },
      "source": [
        "## Functions for pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MthVK7EQm6m_"
      },
      "source": [
        "### _get_train_data\n",
        "\n",
        "This function generates training data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPGq-Y1Jmvv5"
      },
      "source": [
        "def _get_train_data( data_location, file_name, include_context, include_idiom ) :\n",
        "\n",
        "    file_name = os.path.join( data_location, file_name )\n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem in data :\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem[ header.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ header.index( 'Previous' ) ], elem[ header.index( 'Target' ) ], elem[ header.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ]\n",
        "        else :\n",
        "            sentence2 = elem[ header.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        out_data.append( this_row )\n",
        "        assert len( out_header ) == len( this_row )\n",
        "    return [ out_header ] + out_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytociCB3WZM"
      },
      "source": [
        "### _get_dev_eval_data\n",
        "\n",
        "This function generates training dev and eval data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters.\n",
        "\n",
        "Additionally, if there is no gold label provides (as in the case of eval) it will generate a file that can be used to generate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe4YQJ9Sm-B2"
      },
      "source": [
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None :\n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None :\n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "\n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "\n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ]\n",
        "        else :\n",
        "            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        assert len( out_header ) == len( this_row )\n",
        "        out_data.append( this_row )\n",
        "\n",
        "\n",
        "    return [ out_header ] + out_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIbyTnn3fHP"
      },
      "source": [
        "### create_data\n",
        "\n",
        "This function generates the training, development and evaluation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1tr-zNvnBCV"
      },
      "source": [
        "def create_data( input_location, output_location ) :\n",
        "\n",
        "\n",
        "    ## Zero shot data\n",
        "    train_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot_balanced.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False\n",
        "    )\n",
        "    write_csv( train_data, os.path.join( output_location, 'ZeroShot', 'train.csv' ) )\n",
        "\n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv',\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "\n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None ,\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "\n",
        "    ## OneShot Data (combine both for training)\n",
        "    train_zero_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot_balanced.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "    train_one_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "\n",
        "    assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "    train_data = train_one_data + train_zero_data[1:]\n",
        "    write_csv( train_data, os.path.join( output_location, 'OneShot', 'train.csv' ) )\n",
        "\n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv',\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\n",
        "    write_csv( dev_data, os.path.join( output_location, 'OneShot', 'dev.csv' ) )\n",
        "\n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'OneShot', 'eval.csv' ) )\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmQfvym8ndKH"
      },
      "source": [
        "## Setup and Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxCgaHlKnpMR",
        "outputId": "fa95efc6-3eec-4c67-e01b-0f51a2f0d233"
      },
      "source": [
        "zero_shot = pd.read_csv('/content/Idiomacity-Detection/Rawdata/train_zero_shot_balanced.csv')\n",
        "print('Zero-shot Language Labels\\n' , zero_shot['Language'].value_counts())\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "one_shot = pd.read_csv('/content/Idiomacity-Detection/Rawdata/train_one_shot_balanced.csv')\n",
        "print('One-shot Language Labels\\n' , one_shot['Language'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot Language Labels\n",
            " EN    1164\n",
            "PT    1164\n",
            "Name: Language, dtype: int64\n",
            "\n",
            "\n",
            "One-shot Language Labels\n",
            " EN    53\n",
            "PT    53\n",
            "Name: Language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkeKLg-Hngs4",
        "outputId": "3fc1adac-d43b-4b51-fcb4-cf22dd1538bf"
      },
      "source": [
        "outpath = 'Data'\n",
        "\n",
        "Path( os.path.join( outpath, 'ZeroShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "Path( os.path.join( outpath, 'OneShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "create_data('/content/Idiomacity-Detection/Rawdata/', outpath )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/train.csv\n",
            "Wrote Data/ZeroShot/dev.csv\n",
            "Wrote Data/ZeroShot/eval.csv\n",
            "Wrote Data/OneShot/train.csv\n",
            "Wrote Data/OneShot/dev.csv\n",
            "Wrote Data/OneShot/eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-Ol7hfoC8a"
      },
      "source": [
        "# Zero Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-GiQvnkoL67"
      },
      "source": [
        "## Train Zero shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k0BwA0uoKAu",
        "outputId": "1cbb87af-9bd1-4df7-cab3-82eb2f0f3e9d"
      },
      "source": [
        "!python /content/Idiomacity-Detection/py_codes/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'xlm-roberta-large' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 17:12:37.194416: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Jan13_17-12-41_99929c0811bd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/ZeroShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/ZeroShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-82fb9426943cd361\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-82fb9426943cd361/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10106.76it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1257.85it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-82fb9426943cd361/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 777.23it/s]\n",
            "Downloading: 100% 616/616 [00:00<00:00, 616kB/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-13 17:12:44,992 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:12:44,995 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:449] 2023-01-13 17:12:45,915 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2023-01-13 17:12:46,817 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:12:46,817 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Downloading: 100% 5.07M/5.07M [00:01<00:00, 2.81MB/s]\n",
            "Downloading: 100% 9.10M/9.10M [00:01<00:00, 4.57MB/s]\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:12:57,548 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:12:57,548 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:12:57,549 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:12:57,549 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:12:57,549 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2023-01-13 17:12:57,549 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:12:57,550 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Downloading: 100% 2.24G/2.24G [00:28<00:00, 78.8MB/s]\n",
            "[INFO|modeling_utils.py:2204] 2023-01-13 17:13:27,578 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2698] 2023-01-13 17:13:33,756 >> Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2710] 2023-01-13 17:13:33,756 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 3/3 [00:00<00:00, 12.52ba/s]\n",
            "100% 1/1 [00:00<00:00, 14.42ba/s]\n",
            "INFO:__main__:Sample 1577 of the training set: {'label': 1, 'sentence1': 'Começamos no ano passado por causa da pandemia e, desde então, quando o estoque de alimentos do serviço social abaixa ou é insuficiente para a demanda, nós fazemos o drive\", explica Diego Narvaes, empresário e voluntário da instituição. Das 9h às 15h, a população poderá dirigir e fazer suas doações no Salão Paroquial, localizado na esquina da rua Ondina com a rua Generosa Bastos, na Redentora. As doações acontecerão respeitando os protocolos de segurança e o distanciamento social, de acordo com a organização do evento.', 'input_ids': [0, 10592, 2968, 840, 110, 2373, 49523, 196, 7278, 48, 115203, 7605, 28, 4, 3287, 27573, 4, 3406, 36, 13124, 944, 8, 36797, 54, 44165, 2265, 10, 111857, 628, 393, 185438, 13, 121, 10, 32481, 4, 17724, 60567, 232, 36, 22648, 830, 14149, 59826, 152083, 90, 4, 152932, 28, 87093, 9924, 48, 153785, 5, 1858, 483, 127, 11845, 423, 127, 4, 10, 80805, 35610, 136570, 28, 7255, 9036, 54, 24489, 110, 6565, 3680, 69456, 3181, 289, 4, 112682, 24, 198, 55869, 48, 64700, 2161, 10669, 375, 10, 64700, 88342, 4244, 12506, 1952, 4, 24, 6096, 33, 29619, 5, 1301, 54, 24489, 100648, 3680, 178240, 557, 362, 167883, 7, 8, 43025, 28, 36, 73274, 2611, 2265, 4, 8, 18172, 375, 10, 96582, 54, 15058, 5, 2, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 1722 of the training set: {'label': 1, 'sentence1': 'O Relatório de Pesquisa de Mercado Piscina da escada rolante fornece estatísticas vitais exclusivas, dados, informações, tendências e detalhes do cenário competitivo neste nicho de setor. O relatório de pesquisa da indústria global Piscina da escada rolante Mercado é um estudo profissional e aprofundado sobre o tamanho do mercado, crescimento, participação, tendências, bem como análise da indústria. De acordo com os detalhes dos números de consumo, a previsão do mercado global Piscina da escada rolante para 2026 .', 'input_ids': [0, 180, 29690, 31757, 8, 140028, 8, 113477, 32542, 15347, 48, 38940, 85, 6136, 2733, 95049, 208376, 7, 4971, 164, 99605, 7, 4, 26620, 4, 22979, 4, 130893, 7, 28, 99301, 54, 129205, 165527, 8512, 6445, 31, 8, 84372, 5, 180, 142060, 8, 31639, 48, 99136, 7964, 32542, 15347, 48, 38940, 85, 6136, 2733, 113477, 393, 286, 43236, 50941, 28, 229647, 246, 1028, 36, 104240, 54, 10616, 4, 98963, 4, 86354, 4, 130893, 7, 4, 5289, 533, 53274, 48, 99136, 5, 262, 18172, 375, 362, 99301, 655, 71891, 8, 26834, 4, 10, 479, 75801, 54, 10616, 7964, 32542, 15347, 48, 38940, 85, 6136, 2733, 121, 387, 4046, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 165 of the training set: {'label': 1, 'sentence1': 'Rest assured, they are real, and they bring a unique take to the genre. Recently, fine line tattoos have been increasing in popularity among residents of Toronto, Markham, Mississauga and Kitsilano. But it’s not just a local trend.', 'input_ids': [0, 49756, 92784, 71, 4, 1836, 621, 2773, 4, 136, 1836, 19095, 10, 36998, 5646, 47, 70, 30773, 5, 169549, 538, 4, 5885, 13315, 79276, 7, 765, 2809, 118055, 23, 5700, 2481, 54940, 160641, 111, 90915, 4, 7880, 3915, 4, 16771, 3031, 16565, 136, 23564, 38707, 157, 5, 4966, 442, 26, 7, 959, 1660, 10, 4000, 12768, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:703] 2023-01-13 17:13:39,964 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1634] 2023-01-13 17:13:39,974 >> ***** Running training *****\n",
            "[INFO|trainer.py:1635] 2023-01-13 17:13:39,974 >>   Num examples = 2328\n",
            "[INFO|trainer.py:1636] 2023-01-13 17:13:39,975 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1637] 2023-01-13 17:13:39,975 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1638] 2023-01-13 17:13:39,975 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1639] 2023-01-13 17:13:39,975 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1640] 2023-01-13 17:13:39,975 >>   Total optimization steps = 657\n",
            "[INFO|trainer.py:1641] 2023-01-13 17:13:39,976 >>   Number of trainable parameters = 559892482\n",
            " 11% 73/657 [00:43<05:02,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:14:23,453 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:14:23,455 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:14:23,455 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:14:23,455 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.15it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.48it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.94it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.33it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.98it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.75it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.60it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.55it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.48it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.42it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.39it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.38it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.38it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.39it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.38it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.40it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.38it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.35it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.35it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.35it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.37it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.35it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.36it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.39it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9098325967788696, 'eval_accuracy': 0.5588633418083191, 'eval_f1': 0.5188955810461067, 'eval_runtime': 4.7891, 'eval_samples_per_second': 154.31, 'eval_steps_per_second': 19.419, 'epoch': 1.0}\n",
            " 11% 73/657 [00:48<05:02,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:14:28,246 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-73\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:14:28,247 >> Configuration saved in models/ZeroShot/0/checkpoint-73/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:14:31,898 >> Model weights saved in models/ZeroShot/0/checkpoint-73/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:14:31,898 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-73/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:14:31,899 >> Special tokens file saved in models/ZeroShot/0/checkpoint-73/special_tokens_map.json\n",
            " 22% 146/657 [01:43<04:24,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:15:23,718 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:15:23,719 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:15:23,719 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:15:23,719 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.30it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.53it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.01it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.36it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.96it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.72it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.65it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.61it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.54it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.47it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.45it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.40it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.34it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.32it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.36it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.38it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.38it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.38it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.39it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.49it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.50it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.49it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.40it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.40it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.35it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.34it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.34it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7360919117927551, 'eval_accuracy': 0.6630581617355347, 'eval_f1': 0.6630483148845002, 'eval_runtime': 4.7704, 'eval_samples_per_second': 154.915, 'eval_steps_per_second': 19.495, 'epoch': 2.0}\n",
            " 22% 146/657 [01:48<04:24,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:15:28,491 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-146\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:15:28,492 >> Configuration saved in models/ZeroShot/0/checkpoint-146/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:15:31,974 >> Model weights saved in models/ZeroShot/0/checkpoint-146/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:15:31,974 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-146/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:15:31,975 >> Special tokens file saved in models/ZeroShot/0/checkpoint-146/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:15:43,621 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-73] due to args.save_total_limit\n",
            " 33% 219/657 [02:44<03:46,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:16:24,724 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:16:24,726 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:16:24,726 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:16:24,726 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.15it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.54it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.97it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.35it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.02it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.82it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.71it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.66it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.62it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.54it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.49it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.43it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.41it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.42it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.30it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.29it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.32it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.40it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.40it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.42it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.41it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0770646333694458, 'eval_accuracy': 0.6454668641090393, 'eval_f1': 0.6300198731177864, 'eval_runtime': 4.7687, 'eval_samples_per_second': 154.968, 'eval_steps_per_second': 19.502, 'epoch': 3.0}\n",
            " 33% 219/657 [02:49<03:46,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:16:29,496 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-219\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:16:29,497 >> Configuration saved in models/ZeroShot/0/checkpoint-219/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:16:32,926 >> Model weights saved in models/ZeroShot/0/checkpoint-219/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:16:32,927 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-219/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:16:32,927 >> Special tokens file saved in models/ZeroShot/0/checkpoint-219/special_tokens_map.json\n",
            " 44% 292/657 [03:44<03:08,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:17:24,968 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:17:24,970 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:17:24,970 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:17:24,970 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.35it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.65it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.01it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.30it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.97it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.58it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.54it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.54it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.49it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.45it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.40it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.36it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.31it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.28it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.27it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.29it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.30it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.33it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.36it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.36it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.35it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.37it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.37it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.37it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.40it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.43it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.36it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.34it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.31it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.31it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.30it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.32it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.35it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.706108808517456, 'eval_accuracy': 0.6738836169242859, 'eval_f1': 0.6734147875533845, 'eval_runtime': 4.7811, 'eval_samples_per_second': 154.567, 'eval_steps_per_second': 19.452, 'epoch': 4.0}\n",
            " 44% 292/657 [03:49<03:08,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:17:29,753 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-292\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:17:29,754 >> Configuration saved in models/ZeroShot/0/checkpoint-292/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:17:33,148 >> Model weights saved in models/ZeroShot/0/checkpoint-292/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:17:33,149 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-292/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:17:33,149 >> Special tokens file saved in models/ZeroShot/0/checkpoint-292/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:17:45,003 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-146] due to args.save_total_limit\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:17:45,841 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-219] due to args.save_total_limit\n",
            " 56% 365/657 [04:47<02:31,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:18:26,999 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:18:27,001 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:18:27,001 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:18:27,001 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.17it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.59it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.03it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.35it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.03it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.84it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.69it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.63it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.60it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.48it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.43it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.40it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.38it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.35it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.32it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.33it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.34it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.33it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.33it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.33it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.37it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.35it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.35it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.37it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.37it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.40it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.43it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.42it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.8716844320297241, 'eval_accuracy': 0.6874154210090637, 'eval_f1': 0.6840740637925771, 'eval_runtime': 4.7709, 'eval_samples_per_second': 154.897, 'eval_steps_per_second': 19.493, 'epoch': 5.0}\n",
            " 56% 365/657 [04:51<02:31,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:18:31,773 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-365\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:18:31,774 >> Configuration saved in models/ZeroShot/0/checkpoint-365/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:18:35,180 >> Model weights saved in models/ZeroShot/0/checkpoint-365/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:18:35,181 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-365/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:18:35,181 >> Special tokens file saved in models/ZeroShot/0/checkpoint-365/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:18:46,934 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-292] due to args.save_total_limit\n",
            " 67% 438/657 [05:48<01:53,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:19:28,095 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:19:28,096 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:19:28,097 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:19:28,097 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.18it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.45it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.88it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 20.20it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.85it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.69it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.61it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.54it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.45it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.43it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.39it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.36it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.36it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 18.97it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.07it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.20it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.29it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.36it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.37it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.38it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.34it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.33it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.35it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.40it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.40it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.41it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.39it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.41it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.0477564334869385, 'eval_accuracy': 0.6928281188011169, 'eval_f1': 0.6928258962822985, 'eval_runtime': 4.789, 'eval_samples_per_second': 154.313, 'eval_steps_per_second': 19.42, 'epoch': 6.0}\n",
            " 67% 438/657 [05:52<01:53,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:19:32,888 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-438\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:19:32,889 >> Configuration saved in models/ZeroShot/0/checkpoint-438/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:19:36,356 >> Model weights saved in models/ZeroShot/0/checkpoint-438/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:19:36,357 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-438/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:19:36,357 >> Special tokens file saved in models/ZeroShot/0/checkpoint-438/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:19:48,001 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-365] due to args.save_total_limit\n",
            "{'loss': 0.24, 'learning_rate': 4.779299847792998e-06, 'epoch': 6.85}\n",
            " 78% 511/657 [06:49<01:15,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:20:29,201 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:20:29,203 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:20:29,203 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:20:29,203 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.10it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.44it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.89it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 20.24it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.93it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.75it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.67it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.47it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.44it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.42it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.40it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.39it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.41it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.42it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.38it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.38it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.39it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.37it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.37it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.37it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.35it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.2944753170013428, 'eval_accuracy': 0.6955345273017883, 'eval_f1': 0.695478745322982, 'eval_runtime': 4.7757, 'eval_samples_per_second': 154.743, 'eval_steps_per_second': 19.474, 'epoch': 7.0}\n",
            " 78% 511/657 [06:53<01:15,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:20:33,980 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-511\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:20:33,981 >> Configuration saved in models/ZeroShot/0/checkpoint-511/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:20:37,473 >> Model weights saved in models/ZeroShot/0/checkpoint-511/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:20:37,474 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-511/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:20:37,474 >> Special tokens file saved in models/ZeroShot/0/checkpoint-511/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:20:49,084 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-438] due to args.save_total_limit\n",
            " 89% 584/657 [07:50<00:37,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:21:30,182 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:21:30,184 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:21:30,184 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:21:30,184 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.16it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.49it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.95it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.33it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.98it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.76it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.65it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.48it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.42it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.39it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.40it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.42it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.41it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.42it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.42it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.42it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.39it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.40it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.41it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.43it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.44it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.41it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.44it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.44it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.37it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.40it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.4042837619781494, 'eval_accuracy': 0.6928281188011169, 'eval_f1': 0.6926458005448912, 'eval_runtime': 4.7716, 'eval_samples_per_second': 154.876, 'eval_steps_per_second': 19.49, 'epoch': 8.0}\n",
            " 89% 584/657 [07:54<00:37,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:21:34,957 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-584\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:21:34,957 >> Configuration saved in models/ZeroShot/0/checkpoint-584/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:21:38,579 >> Model weights saved in models/ZeroShot/0/checkpoint-584/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:21:38,580 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-584/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:21:38,580 >> Special tokens file saved in models/ZeroShot/0/checkpoint-584/special_tokens_map.json\n",
            "100% 657/657 [08:50<00:00,  1.93it/s][INFO|trainer.py:703] 2023-01-13 17:22:30,317 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:22:30,318 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:22:30,318 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:22:30,318 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.26it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.35it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.86it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 20.23it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 19.57it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:03, 19.53it/s]\u001b[A\n",
            " 20% 19/93 [00:00<00:03, 19.52it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.47it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.44it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.41it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.39it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.37it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.37it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.32it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.37it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.34it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.31it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.31it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.31it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.33it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:01, 19.35it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.34it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.31it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.32it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.37it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.38it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.30it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.35it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.34it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.35it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.36it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.34222674369812, 'eval_accuracy': 0.6820027232170105, 'eval_f1': 0.6805403028948477, 'eval_runtime': 4.7885, 'eval_samples_per_second': 154.327, 'eval_steps_per_second': 19.421, 'epoch': 9.0}\n",
            "100% 657/657 [08:55<00:00,  1.93it/s]\n",
            "100% 93/93 [00:04<00:00, 19.35it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:22:35,108 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-657\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:22:35,109 >> Configuration saved in models/ZeroShot/0/checkpoint-657/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:22:38,687 >> Model weights saved in models/ZeroShot/0/checkpoint-657/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:22:38,688 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-657/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:22:38,688 >> Special tokens file saved in models/ZeroShot/0/checkpoint-657/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:22:50,326 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-584] due to args.save_total_limit\n",
            "[INFO|trainer.py:1885] 2023-01-13 17:22:51,169 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2009] 2023-01-13 17:22:51,169 >> Loading best model from models/ZeroShot/0/checkpoint-511 (score: 0.695478745322982).\n",
            "{'train_runtime': 553.0207, 'train_samples_per_second': 37.886, 'train_steps_per_second': 1.188, 'train_loss': 0.183570749229855, 'epoch': 9.0}\n",
            "100% 657/657 [09:13<00:00,  1.93it/s][INFO|trainer.py:1919] 2023-01-13 17:22:52,998 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-657] due to args.save_total_limit\n",
            "100% 657/657 [09:13<00:00,  1.19it/s]\n",
            "[INFO|trainer.py:2693] 2023-01-13 17:22:53,847 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:22:53,848 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:22:58,655 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:22:58,656 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:22:58,656 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.1836\n",
            "  train_runtime            = 0:09:13.02\n",
            "  train_samples            =       2328\n",
            "  train_samples_per_second =     37.886\n",
            "  train_steps_per_second   =      1.188\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:703] 2023-01-13 17:22:58,727 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:22:58,729 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:22:58,729 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:22:58,730 >>   Batch size = 8\n",
            "100% 93/93 [00:04<00:00, 19.59it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.6955\n",
            "  eval_f1                 =     0.6955\n",
            "  eval_loss               =     2.2945\n",
            "  eval_runtime            = 0:00:04.81\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    153.591\n",
            "  eval_steps_per_second   =     19.329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrfwDiORPDyS"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibb2Uo0vPPc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f6294c-5c04-460c-c67c-e3da3387e3ce"
      },
      "source": [
        "## Create save path\n",
        "# !mkdir -p /content/gdrive/MyDrive/ColabData/Idimaticity_Detection/ZeroShot/0/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/ZeroShot/0/ #* /content/gdrive/MyDrive/ColabData/Idimaticity_Detection/ZeroShot/0/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: missing destination file operand after '/content/models/ZeroShot/0/'\n",
            "Try 'cp --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bN4iUHWP45b"
      },
      "source": [
        "## Evaluation On Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "houOZpcYO-Pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3dd005-56f3-43f7-bd3d-b16385373ae8"
      },
      "source": [
        "!python /content/Idiomacity-Detection/py_codes/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/ZeroShot/0' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/eval-dev/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "      --test_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 17:23:07.763028: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/eval-dev/runs/Jan13_17-23-11_99929c0811bd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/ZeroShot/0/eval-dev/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/eval-dev/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/ZeroShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/ZeroShot/dev.csv\n",
            "INFO:__main__:load a local file for test: Data/ZeroShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-053aba9852f04e72\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-053aba9852f04e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 12216.42it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1602.92it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-053aba9852f04e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 1024.67it/s]\n",
            "[INFO|configuration_utils.py:652] 2023-01-13 17:23:12,231 >> loading configuration file /content/models/ZeroShot/0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:23:12,238 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"/content/models/ZeroShot/0\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:23:12,240 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:23:12,240 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:23:12,240 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:23:12,241 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:23:12,241 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2201] 2023-01-13 17:23:12,893 >> loading weights file /content/models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2708] 2023-01-13 17:23:18,777 >> All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:2716] 2023-01-13 17:23:18,777 >> All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at /content/models/ZeroShot/0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
            "100% 3/3 [00:00<00:00, 13.18ba/s]\n",
            "100% 1/1 [00:00<00:00, 14.95ba/s]\n",
            "100% 1/1 [00:00<00:00, 15.45ba/s]\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:703] 2023-01-13 17:23:23,764 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:23:23,766 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:23:23,766 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:23:23,766 >>   Batch size = 8\n",
            "100% 93/93 [00:04<00:00, 19.56it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6955\n",
            "  eval_f1                 =     0.6955\n",
            "  eval_loss               =     2.2945\n",
            "  eval_runtime            = 0:00:06.87\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    107.461\n",
            "  eval_steps_per_second   =     13.524\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:703] 2023-01-13 17:23:30,647 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:23:30,648 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:23:30,648 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:23:30,648 >>   Batch size = 8\n",
            "100% 93/93 [00:04<00:00, 19.77it/s]\n",
            "INFO:__main__:***** Test results None *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHqPYuTS3muJ"
      },
      "source": [
        "### Use predictions to create the submission file (for dev data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWUuwg7Qm-t"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/Idiomacity-Detection/Rawdata/dev_submission_format.csv' ,\n",
        "    'input_file'             : '/content/Idiomacity-Detection/Rawdata/dev.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/ZeroShot/0/eval-dev/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'zero_shot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgFGlGnTROJZ"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXcRbv70RZfR"
      },
      "source": [
        "!mkdir -p outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ezIzyWTRePp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f5963f-713d-4378-faf8-ad9955503d76"
      },
      "source": [
        "write_csv( updated_data, 'outputs/zero_shot_dev_formated.csv' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/zero_shot_dev_formated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZpFkOSQ3x_A"
      },
      "source": [
        "### For the development data, we can run evaluation script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn6MyP9jRnFA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "9ecd557e-0ed3-4e31-fa23-4de56ad43eab"
      },
      "source": [
        "import sys\n",
        "sys.path.append( '/content/Idiomacity-Detection/py_codes' )\n",
        "from SubTask1Evaluator import evaluate_submission\n",
        "\n",
        "\n",
        "submission_file = 'outputs/zero_shot_dev_formated.csv'\n",
        "gold_file       = '/content/Idiomacity-Detection/Rawdata/dev_gold.csv'\n",
        "\n",
        "results = evaluate_submission( submission_file, gold_file )\n",
        "%reload_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=results[1:], columns=results[0])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Settings Languages  F1 Score (Macro)  Accuracy Score\n",
              "0  zero_shot        EN          0.735067        0.742489\n",
              "1  zero_shot        PT          0.589943        0.615385\n",
              "2  zero_shot     EN,PT          0.695479        0.695535\n",
              "3   one_shot        EN               NaN             NaN\n",
              "4   one_shot        PT               NaN             NaN\n",
              "5   one_shot     EN,PT               NaN             NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d617a235-70ea-4fed-93e4-ab7959bf3878\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "      <th>Accuracy Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.735067</td>\n",
              "      <td>0.742489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.589943</td>\n",
              "      <td>0.615385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.695479</td>\n",
              "      <td>0.695535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d617a235-70ea-4fed-93e4-ab7959bf3878')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d617a235-70ea-4fed-93e4-ab7959bf3878 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d617a235-70ea-4fed-93e4-ab7959bf3878');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n{\n            'v': 0.7350667070952093,\n            'f': \"0.7350667070952093\",\n        },\n{\n            'v': 0.7424892703862661,\n            'f': \"0.7424892703862661\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n{\n            'v': 0.5899434947428653,\n            'f': \"0.5899434947428653\",\n        },\n{\n            'v': 0.6153846153846154,\n            'f': \"0.6153846153846154\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n{\n            'v': 0.695478745322982,\n            'f': \"0.695478745322982\",\n        },\n{\n            'v': 0.6955345060893099,\n            'f': \"0.6955345060893099\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n{\n            'v': NaN,\n            'f': \"NaN\",\n        },\n{\n            'v': NaN,\n            'f': \"NaN\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"number\", \"F1 Score (Macro)\"], [\"number\", \"Accuracy Score\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7Irn_YvIni"
      },
      "source": [
        "# One Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVYvUH3OvR9b"
      },
      "source": [
        "## Train One shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQO751yzvVJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1114163-a3be-49a4-dd39-ccc721ed5f1f"
      },
      "source": [
        "!python /content/Idiomacity-Detection/py_codes/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'xlm-roberta-large' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 17:23:38.694283: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Jan13_17-23-41_99929c0811bd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/OneShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/OneShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-49cb0769efdc570f\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-49cb0769efdc570f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 11155.06it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1457.37it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-49cb0769efdc570f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 990.98it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-13 17:23:43,474 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:23:43,477 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:449] 2023-01-13 17:23:44,388 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:654] 2023-01-13 17:23:45,289 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:23:45,289 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:23:47,108 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:23:47,108 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:23:47,108 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:23:47,108 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-13 17:23:47,108 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:654] 2023-01-13 17:23:47,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:23:47,109 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2204] 2023-01-13 17:23:47,726 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2698] 2023-01-13 17:24:02,650 >> Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2710] 2023-01-13 17:24:02,650 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00, 47.29ba/s]\n",
            "100% 1/1 [00:00<00:00, 21.18ba/s]\n",
            "INFO:__main__:Sample 34 of the training set: {'label': 1, 'sentence1': \"Children of Jaws' might have been a better title for this teen version of the big fish story.\", 'sentence2': 'fish story', 'input_ids': [0, 91345, 111, 823, 19725, 25, 13648, 765, 2809, 10, 11522, 44759, 100, 903, 7003, 11389, 111, 70, 6957, 67155, 13765, 5, 2, 2, 67155, 13765, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 145 of the training set: {'label': 0, 'sentence1': 'Leinster have become such a big fish in such a small pool that something will have to give.', 'sentence2': 'big fish', 'input_ids': [0, 636, 73, 1515, 765, 24209, 6044, 10, 6957, 67155, 23, 6044, 10, 19336, 19361, 450, 9844, 1221, 765, 47, 8337, 5, 2, 2, 6957, 67155, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 216 of the training set: {'label': 1, 'sentence1': 'Atualmente, ele trabalhava na Diretoria de Pessoal Militar (DPM), mas já serviu no Gtop 21. Também foi professor do Colégio Militar Tiradentes.', 'sentence2': 'colégio militar', 'input_ids': [0, 210110, 4, 3163, 68257, 330, 24, 204919, 399, 8, 106616, 141, 67485, 15, 397, 20266, 247, 1163, 3113, 16486, 34, 110, 527, 13784, 7245, 87328, 1715, 16030, 54, 11554, 446, 9702, 67485, 141031, 63398, 7, 5, 2, 2, 3365, 446, 9702, 33796, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:703] 2023-01-13 17:24:05,676 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1634] 2023-01-13 17:24:05,686 >> ***** Running training *****\n",
            "[INFO|trainer.py:1635] 2023-01-13 17:24:05,686 >>   Num examples = 246\n",
            "[INFO|trainer.py:1636] 2023-01-13 17:24:05,686 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1637] 2023-01-13 17:24:05,686 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1638] 2023-01-13 17:24:05,686 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1639] 2023-01-13 17:24:05,686 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1640] 2023-01-13 17:24:05,686 >>   Total optimization steps = 72\n",
            "[INFO|trainer.py:1641] 2023-01-13 17:24:05,687 >>   Number of trainable parameters = 559892482\n",
            " 11% 8/72 [00:05<00:34,  1.84it/s][INFO|trainer.py:703] 2023-01-13 17:24:11,376 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:24:11,378 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:24:11,378 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:24:11,378 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.22it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.50it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 20.98it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.35it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.02it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.86it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.74it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.66it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.51it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.52it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.52it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.48it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.48it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.50it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.50it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.33it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.35it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.39it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.41it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.44it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.49it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.48it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.46it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.49it/s]\u001b[A\n",
            "{'eval_loss': 0.7208014130592346, 'eval_accuracy': 0.5453315377235413, 'eval_f1': 0.3528896672504378, 'eval_runtime': 4.7561, 'eval_samples_per_second': 155.38, 'eval_steps_per_second': 19.554, 'epoch': 1.0}\n",
            "\n",
            " 11% 8/72 [00:10<00:34,  1.84it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:24:16,135 >> Saving model checkpoint to models/OneShot/1/checkpoint-8\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:24:16,136 >> Configuration saved in models/OneShot/1/checkpoint-8/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:24:19,779 >> Model weights saved in models/OneShot/1/checkpoint-8/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:24:19,780 >> tokenizer config file saved in models/OneShot/1/checkpoint-8/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:24:19,780 >> Special tokens file saved in models/OneShot/1/checkpoint-8/special_tokens_map.json\n",
            " 22% 16/72 [00:29<00:55,  1.00it/s][INFO|trainer.py:703] 2023-01-13 17:24:35,484 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:24:35,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:24:35,486 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:24:35,486 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.29it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.56it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.04it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.40it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.01it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.78it/s]\u001b[A\n",
            " 22% 20/93 [00:00<00:03, 19.70it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:03, 19.63it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:03, 19.59it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:03, 19.56it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 19.52it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 19.46it/s]\u001b[A\n",
            " 39% 36/93 [00:01<00:02, 19.43it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:02, 19.41it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:02, 19.42it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 19.48it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 19.50it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 19.50it/s]\u001b[A\n",
            " 56% 52/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 58% 54/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 60% 56/93 [00:02<00:01, 19.44it/s]\u001b[A\n",
            " 62% 58/93 [00:02<00:01, 19.45it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 73% 68/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 75% 70/93 [00:03<00:01, 19.40it/s]\u001b[A\n",
            " 77% 72/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 80% 74/93 [00:03<00:00, 19.43it/s]\u001b[A\n",
            " 82% 76/93 [00:03<00:00, 19.44it/s]\u001b[A\n",
            " 84% 78/93 [00:03<00:00, 19.46it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 19.50it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 19.51it/s]\u001b[A\n",
            " 92% 86/93 [00:04<00:00, 19.49it/s]\u001b[A\n",
            " 95% 88/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 97% 90/93 [00:04<00:00, 19.44it/s]\u001b[A\n",
            " 99% 92/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            "{'eval_loss': 0.6841546893119812, 'eval_accuracy': 0.5548037886619568, 'eval_f1': 0.3754742425604603, 'eval_runtime': 4.7571, 'eval_samples_per_second': 155.345, 'eval_steps_per_second': 19.55, 'epoch': 2.0}\n",
            "\n",
            " 22% 16/72 [00:34<00:55,  1.00it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:24:40,244 >> Saving model checkpoint to models/OneShot/1/checkpoint-16\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:24:40,245 >> Configuration saved in models/OneShot/1/checkpoint-16/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:24:43,423 >> Model weights saved in models/OneShot/1/checkpoint-16/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:24:43,424 >> tokenizer config file saved in models/OneShot/1/checkpoint-16/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:24:43,424 >> Special tokens file saved in models/OneShot/1/checkpoint-16/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:24:55,603 >> Deleting older checkpoint [models/OneShot/1/checkpoint-8] due to args.save_total_limit\n",
            " 33% 24/72 [00:55<00:50,  1.05s/it][INFO|trainer.py:703] 2023-01-13 17:25:00,774 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:25:00,776 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:25:00,776 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:25:00,776 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.19it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.60it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.03it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.39it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.05it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.83it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.71it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.66it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.61it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.55it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.54it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.50it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.47it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.46it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.47it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.48it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.49it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.49it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.49it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.42it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.41it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.43it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.43it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.50it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.49it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.44it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.46it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.49it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.49it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            "{'eval_loss': 0.6811639666557312, 'eval_accuracy': 0.5683355927467346, 'eval_f1': 0.46717611772975587, 'eval_runtime': 4.7535, 'eval_samples_per_second': 155.464, 'eval_steps_per_second': 19.564, 'epoch': 3.0}\n",
            "\n",
            " 33% 24/72 [00:59<00:50,  1.05s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:25:05,531 >> Saving model checkpoint to models/OneShot/1/checkpoint-24\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:25:05,532 >> Configuration saved in models/OneShot/1/checkpoint-24/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:25:08,973 >> Model weights saved in models/OneShot/1/checkpoint-24/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:25:08,974 >> tokenizer config file saved in models/OneShot/1/checkpoint-24/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:25:08,974 >> Special tokens file saved in models/OneShot/1/checkpoint-24/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:25:20,633 >> Deleting older checkpoint [models/OneShot/1/checkpoint-16] due to args.save_total_limit\n",
            " 44% 32/72 [01:20<00:42,  1.05s/it][INFO|trainer.py:703] 2023-01-13 17:25:25,843 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:25:25,845 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:25:25,845 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:25:25,845 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.22it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.52it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 21.00it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.37it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.06it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.84it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.72it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.66it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.50it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.48it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.48it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.45it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.45it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.39it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.44it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.47it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.47it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.44it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.49it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.49it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.43it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.44it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.41it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.43it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.46it/s]\u001b[A\n",
            "{'eval_loss': 0.677004337310791, 'eval_accuracy': 0.5832205414772034, 'eval_f1': 0.57840260798696, 'eval_runtime': 4.7572, 'eval_samples_per_second': 155.343, 'eval_steps_per_second': 19.549, 'epoch': 4.0}\n",
            "\n",
            " 44% 32/72 [01:24<00:42,  1.05s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:25:30,603 >> Saving model checkpoint to models/OneShot/1/checkpoint-32\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:25:30,604 >> Configuration saved in models/OneShot/1/checkpoint-32/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:25:33,948 >> Model weights saved in models/OneShot/1/checkpoint-32/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:25:33,949 >> tokenizer config file saved in models/OneShot/1/checkpoint-32/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:25:33,949 >> Special tokens file saved in models/OneShot/1/checkpoint-32/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:25:45,843 >> Deleting older checkpoint [models/OneShot/1/checkpoint-24] due to args.save_total_limit\n",
            " 56% 40/72 [01:45<00:33,  1.06s/it][INFO|trainer.py:703] 2023-01-13 17:25:51,057 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:25:51,059 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:25:51,059 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:25:51,059 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.18it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.61it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.10it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.45it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.05it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.85it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.71it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.66it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.63it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.59it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.56it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.50it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.46it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.45it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.43it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.49it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.50it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.48it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.46it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.43it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.44it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.49it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.50it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.49it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.43it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.45it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.49it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.51it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.52it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.49it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            "{'eval_loss': 0.704563558101654, 'eval_accuracy': 0.5764546394348145, 'eval_f1': 0.5645879334459007, 'eval_runtime': 4.7531, 'eval_samples_per_second': 155.477, 'eval_steps_per_second': 19.566, 'epoch': 5.0}\n",
            "\n",
            " 56% 40/72 [01:50<00:33,  1.06s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:25:55,813 >> Saving model checkpoint to models/OneShot/1/checkpoint-40\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:25:55,814 >> Configuration saved in models/OneShot/1/checkpoint-40/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:25:59,150 >> Model weights saved in models/OneShot/1/checkpoint-40/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:25:59,151 >> tokenizer config file saved in models/OneShot/1/checkpoint-40/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:25:59,151 >> Special tokens file saved in models/OneShot/1/checkpoint-40/special_tokens_map.json\n",
            " 67% 48/72 [02:09<00:24,  1.02s/it][INFO|trainer.py:703] 2023-01-13 17:26:15,013 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:26:15,014 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:26:15,014 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:26:15,014 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.34it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.64it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.08it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.40it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.05it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.86it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.75it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.70it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.65it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.60it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.56it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.50it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.49it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.48it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.50it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.51it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.51it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.50it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.49it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.48it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.50it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.50it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.43it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.43it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.48it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.48it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.50it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.46it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.50it/s]\u001b[A\n",
            "{'eval_loss': 0.6809456944465637, 'eval_accuracy': 0.6197564005851746, 'eval_f1': 0.6189498408154652, 'eval_runtime': 4.7507, 'eval_samples_per_second': 155.557, 'eval_steps_per_second': 19.576, 'epoch': 6.0}\n",
            "\n",
            " 67% 48/72 [02:14<00:24,  1.02s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:26:19,766 >> Saving model checkpoint to models/OneShot/1/checkpoint-48\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:26:19,767 >> Configuration saved in models/OneShot/1/checkpoint-48/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:26:23,540 >> Model weights saved in models/OneShot/1/checkpoint-48/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:26:23,541 >> tokenizer config file saved in models/OneShot/1/checkpoint-48/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:26:23,542 >> Special tokens file saved in models/OneShot/1/checkpoint-48/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:26:35,693 >> Deleting older checkpoint [models/OneShot/1/checkpoint-32] due to args.save_total_limit\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:26:36,595 >> Deleting older checkpoint [models/OneShot/1/checkpoint-40] due to args.save_total_limit\n",
            " 78% 56/72 [02:36<00:17,  1.09s/it][INFO|trainer.py:703] 2023-01-13 17:26:41,852 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:26:41,854 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:26:41,854 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:26:41,854 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.34it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.62it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.08it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.42it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.07it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.87it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.74it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.67it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.63it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.44it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.44it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.44it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.46it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.46it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.40it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.41it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.42it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.42it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.45it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.36it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.38it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.41it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.44it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.44it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.43it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.44it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.46it/s]\u001b[A\n",
            "{'eval_loss': 0.7095033526420593, 'eval_accuracy': 0.6292287111282349, 'eval_f1': 0.6282969130907275, 'eval_runtime': 4.7582, 'eval_samples_per_second': 155.311, 'eval_steps_per_second': 19.545, 'epoch': 7.0}\n",
            "\n",
            " 78% 56/72 [02:40<00:17,  1.09s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:26:46,614 >> Saving model checkpoint to models/OneShot/1/checkpoint-56\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:26:46,615 >> Configuration saved in models/OneShot/1/checkpoint-56/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:26:50,051 >> Model weights saved in models/OneShot/1/checkpoint-56/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:26:50,052 >> tokenizer config file saved in models/OneShot/1/checkpoint-56/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:26:50,052 >> Special tokens file saved in models/OneShot/1/checkpoint-56/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:27:01,675 >> Deleting older checkpoint [models/OneShot/1/checkpoint-48] due to args.save_total_limit\n",
            " 89% 64/72 [03:01<00:08,  1.05s/it][INFO|trainer.py:703] 2023-01-13 17:27:06,832 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:27:06,833 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:27:06,834 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:27:06,834 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.33it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.63it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.06it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.39it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.05it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.86it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.74it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.69it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.62it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.58it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.53it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.49it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.48it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.47it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.39it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.43it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.45it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.44it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.43it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.44it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.47it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.48it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.46it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.46it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.39it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.38it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.41it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.46it/s]\u001b[A\n",
            "{'eval_loss': 0.6912286281585693, 'eval_accuracy': 0.6427605152130127, 'eval_f1': 0.6422095070422534, 'eval_runtime': 4.7558, 'eval_samples_per_second': 155.39, 'eval_steps_per_second': 19.555, 'epoch': 8.0}\n",
            "\n",
            " 89% 64/72 [03:05<00:08,  1.05s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:27:11,591 >> Saving model checkpoint to models/OneShot/1/checkpoint-64\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:27:11,591 >> Configuration saved in models/OneShot/1/checkpoint-64/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:27:14,857 >> Model weights saved in models/OneShot/1/checkpoint-64/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:27:16,522 >> tokenizer config file saved in models/OneShot/1/checkpoint-64/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:27:16,523 >> Special tokens file saved in models/OneShot/1/checkpoint-64/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:27:27,081 >> Deleting older checkpoint [models/OneShot/1/checkpoint-56] due to args.save_total_limit\n",
            "100% 72/72 [03:26<00:00,  1.06s/it][INFO|trainer.py:703] 2023-01-13 17:27:32,298 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:27:32,300 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:27:32,300 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:27:32,300 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 29.15it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:03, 22.59it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:03, 21.05it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:03, 20.41it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:03, 20.05it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:03, 19.85it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:03, 19.70it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:03, 19.65it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 19.61it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 19.58it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 19.57it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 19.55it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 19.51it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:02, 19.49it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:02, 19.48it/s]\u001b[A\n",
            " 42% 39/93 [00:01<00:02, 19.47it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 19.48it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 19.50it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 19.47it/s]\u001b[A\n",
            " 57% 53/93 [00:02<00:02, 19.46it/s]\u001b[A\n",
            " 59% 55/93 [00:02<00:01, 19.47it/s]\u001b[A\n",
            " 61% 57/93 [00:02<00:01, 19.46it/s]\u001b[A\n",
            " 63% 59/93 [00:02<00:01, 19.44it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 19.42it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 19.45it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 19.47it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 19.49it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 19.48it/s]\u001b[A\n",
            " 76% 71/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 78% 73/93 [00:03<00:01, 19.46it/s]\u001b[A\n",
            " 81% 75/93 [00:03<00:00, 19.44it/s]\u001b[A\n",
            " 83% 77/93 [00:03<00:00, 19.43it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 19.44it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 19.45it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 19.50it/s]\u001b[A\n",
            " 94% 87/93 [00:04<00:00, 19.47it/s]\u001b[A\n",
            " 96% 89/93 [00:04<00:00, 19.48it/s]\u001b[A\n",
            " 98% 91/93 [00:04<00:00, 19.46it/s]\u001b[A\n",
            "{'eval_loss': 0.6899908185005188, 'eval_accuracy': 0.6562922596931458, 'eval_f1': 0.6556055424268667, 'eval_runtime': 4.7538, 'eval_samples_per_second': 155.453, 'eval_steps_per_second': 19.563, 'epoch': 9.0}\n",
            "\n",
            "100% 72/72 [03:31<00:00,  1.06s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2693] 2023-01-13 17:27:37,055 >> Saving model checkpoint to models/OneShot/1/checkpoint-72\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:27:37,056 >> Configuration saved in models/OneShot/1/checkpoint-72/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:27:40,379 >> Model weights saved in models/OneShot/1/checkpoint-72/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:27:40,379 >> tokenizer config file saved in models/OneShot/1/checkpoint-72/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:27:40,380 >> Special tokens file saved in models/OneShot/1/checkpoint-72/special_tokens_map.json\n",
            "[INFO|trainer.py:2771] 2023-01-13 17:27:52,102 >> Deleting older checkpoint [models/OneShot/1/checkpoint-64] due to args.save_total_limit\n",
            "[INFO|trainer.py:1885] 2023-01-13 17:27:52,992 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2009] 2023-01-13 17:27:52,992 >> Loading best model from models/OneShot/1/checkpoint-72 (score: 0.6556055424268667).\n",
            "{'train_runtime': 229.2481, 'train_samples_per_second': 9.658, 'train_steps_per_second': 0.314, 'train_loss': 0.5905456013149686, 'epoch': 9.0}\n",
            "100% 72/72 [03:49<00:00,  3.18s/it]\n",
            "[INFO|trainer.py:2693] 2023-01-13 17:27:54,938 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:447] 2023-01-13 17:27:54,939 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1637] 2023-01-13 17:27:59,922 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-13 17:27:59,923 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-13 17:27:59,923 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.5905\n",
            "  train_runtime            = 0:03:49.24\n",
            "  train_samples            =        246\n",
            "  train_samples_per_second =      9.658\n",
            "  train_steps_per_second   =      0.314\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:703] 2023-01-13 17:27:59,978 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:27:59,980 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:27:59,980 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:27:59,980 >>   Batch size = 8\n",
            "100% 93/93 [00:04<00:00, 19.60it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.6563\n",
            "  eval_f1                 =     0.6556\n",
            "  eval_loss               =       0.69\n",
            "  eval_runtime            = 0:00:04.80\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    153.678\n",
            "  eval_steps_per_second   =      19.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW8stSsnIKWo"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0uO16BfcIur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5de017d-6bdc-4b20-f8f4-8d1f2a95b716"
      },
      "source": [
        "## Create save path\n",
        "# !mkdir -p /content/gdrive/MyDrive/ColabData/Idiomacity_Detection/OneShot/1/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/OneShot/1/#* /content/gdrive/MyDrive/ColabData/Idiomacity_Detection/OneShot/1/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/models/OneShot/1/#*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dijAXZ7dD5V"
      },
      "source": [
        "## Evaluation On Dev Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2903I4hKdJuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0cbe75-2137-4d3a-a621-ecde2a8c53e7"
      },
      "source": [
        "!python /content/Idiomacity-Detection/py_codes/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/OneShot/1' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/eval-dev/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "      --test_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 17:28:07.588178: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/eval-dev/runs/Jan13_17-28-10_99929c0811bd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/OneShot/1/eval-dev/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/eval-dev/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/OneShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/OneShot/dev.csv\n",
            "INFO:__main__:load a local file for test: Data/OneShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-15df59b0388f542a\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-15df59b0388f542a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 11554.56it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1523.35it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-15df59b0388f542a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 999.36it/s]\n",
            "[INFO|configuration_utils.py:652] 2023-01-13 17:28:11,784 >> loading configuration file /content/models/OneShot/1/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-13 17:28:11,787 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"/content/models/OneShot/1\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:28:11,788 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:28:11,788 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:28:11,788 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:28:11,788 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1797] 2023-01-13 17:28:11,788 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2201] 2023-01-13 17:28:12,383 >> loading weights file /content/models/OneShot/1/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2708] 2023-01-13 17:28:18,323 >> All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:2716] 2023-01-13 17:28:18,323 >> All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at /content/models/OneShot/1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 45.28ba/s]\n",
            "100% 1/1 [00:00<00:00, 20.55ba/s]\n",
            "100% 1/1 [00:00<00:00, 22.58ba/s]\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:703] 2023-01-13 17:28:22,005 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:28:22,006 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:28:22,006 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:28:22,006 >>   Batch size = 8\n",
            "100% 93/93 [00:04<00:00, 19.65it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6563\n",
            "  eval_f1                 =     0.6556\n",
            "  eval_loss               =       0.69\n",
            "  eval_runtime            = 0:00:06.15\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    120.084\n",
            "  eval_steps_per_second   =     15.112\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:703] 2023-01-13 17:28:28,164 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2944] 2023-01-13 17:28:28,166 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2946] 2023-01-13 17:28:28,166 >>   Num examples = 739\n",
            "[INFO|trainer.py:2949] 2023-01-13 17:28:28,166 >>   Batch size = 8\n",
            "100% 93/93 [00:04<00:00, 19.76it/s]\n",
            "INFO:__main__:***** Test results None *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKjobV-x4R_8"
      },
      "source": [
        "### Use predictions to create the submission file (for dev data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZIEpIstdg_j"
      },
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/outputs/zero_shot_dev_formated.csv' ,\n",
        "    'input_file'             : '/content/Idiomacity-Detection/Rawdata/dev.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/OneShot/1/eval-dev/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'one_shot'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrS0hvEDdspS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9c0a79-6929-44a8-e826-d65879f250f8"
      },
      "source": [
        " updated_data = insert_to_submission_file( **params )\n",
        " write_csv( updated_data, 'outputs/both_dev_formated.csv' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/both_dev_formated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB2cBSmA4ZSR"
      },
      "source": [
        "### For the development data, we can run evaluation script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTWEOw3Qd0Tz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "6cda4b77-2d09-4476-e409-90f8744b2fca"
      },
      "source": [
        "import sys\n",
        "sys.path.append( '/content/Idiomacity-Detection/' )\n",
        "from SubTask1Evaluator import evaluate_submission\n",
        "\n",
        "\n",
        "submission_file = 'outputs/both_dev_formated.csv'\n",
        "gold_file       = '/content/Idiomacity-Detection/Rawdata/dev_gold.csv'\n",
        "\n",
        "results = evaluate_submission( submission_file, gold_file )\n",
        "%reload_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=results[1:], columns=results[0])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Settings Languages  F1 Score (Macro)  Accuracy Score\n",
              "0  zero_shot        EN          0.735067        0.742489\n",
              "1  zero_shot        PT          0.589943        0.615385\n",
              "2  zero_shot     EN,PT          0.695479        0.695535\n",
              "3   one_shot        EN          0.663023        0.667382\n",
              "4   one_shot        PT          0.597411        0.637363\n",
              "5   one_shot     EN,PT          0.655606        0.656292"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-484565a5-db94-40b2-8de1-5ad0ea6ee7f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "      <th>Accuracy Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.735067</td>\n",
              "      <td>0.742489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.589943</td>\n",
              "      <td>0.615385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.695479</td>\n",
              "      <td>0.695535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.663023</td>\n",
              "      <td>0.667382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.597411</td>\n",
              "      <td>0.637363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.655606</td>\n",
              "      <td>0.656292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-484565a5-db94-40b2-8de1-5ad0ea6ee7f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-484565a5-db94-40b2-8de1-5ad0ea6ee7f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-484565a5-db94-40b2-8de1-5ad0ea6ee7f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n{\n            'v': 0.7350667070952093,\n            'f': \"0.7350667070952093\",\n        },\n{\n            'v': 0.7424892703862661,\n            'f': \"0.7424892703862661\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n{\n            'v': 0.5899434947428653,\n            'f': \"0.5899434947428653\",\n        },\n{\n            'v': 0.6153846153846154,\n            'f': \"0.6153846153846154\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n{\n            'v': 0.695478745322982,\n            'f': \"0.695478745322982\",\n        },\n{\n            'v': 0.6955345060893099,\n            'f': \"0.6955345060893099\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n{\n            'v': 0.6630230420766328,\n            'f': \"0.6630230420766328\",\n        },\n{\n            'v': 0.6673819742489271,\n            'f': \"0.6673819742489271\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n{\n            'v': 0.5974111092905128,\n            'f': \"0.5974111092905128\",\n        },\n{\n            'v': 0.6373626373626373,\n            'f': \"0.6373626373626373\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n{\n            'v': 0.6556055424268667,\n            'f': \"0.6556055424268667\",\n        },\n{\n            'v': 0.6562922868741543,\n            'f': \"0.6562922868741543\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"number\", \"F1 Score (Macro)\"], [\"number\", \"Accuracy Score\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "lbCn-ABkykqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import *\n",
        "data_results = pd.read_csv('/content/outputs/both_dev_formated.csv')\n",
        "\n",
        "\n",
        "gold_file_csv = pd.read_csv('/content/Idiomacity-Detection/Rawdata/dev_gold.csv')\n",
        "\n",
        "unbalanced_data = pd.read_csv('/content/Idiomacity-Detection/Model_Results/Results_XLMRLarge.csv')"
      ],
      "metadata": {
        "id": "jN9YRa2AykZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unbalanced"
      ],
      "metadata": {
        "id": "dpbGIuMlU7f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot"
      ],
      "metadata": {
        "id": "ONbJO8lfVBLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_zero_results = unbalanced_data[unbalanced_data['Setting']=='zero_shot']"
      ],
      "metadata": {
        "id": "MJDJq7jVU6WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(gold_file_csv['Label'],u_zero_results['Label']), display_labels = [False, True]).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "LMZ5UrQKVE4P",
        "outputId": "a7927dfb-3665-4889-b040-19e185c4a50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3f8f1d09d0>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-shot"
      ],
      "metadata": {
        "id": "Zca-pRCIVVf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_one_results= unbalanced_data[unbalanced_data['Setting']=='one_shot']"
      ],
      "metadata": {
        "id": "BVXUkN-bVVf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(gold_file_csv['Label'],u_one_results['Label']), display_labels = [False, True]).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "9637d3d6-ad8e-4544-a21c-15cd10df675b",
        "id": "p9ysFnzEVVf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3f8818cbb0>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced"
      ],
      "metadata": {
        "id": "_CoFOC3DVf2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot"
      ],
      "metadata": {
        "id": "NoEoXvIaVf2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_zero_results = data_results[data_results['Setting']=='zero_shot']"
      ],
      "metadata": {
        "id": "t2x0_YUbVf2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(gold_file_csv['Label'],balanced_zero_results['Label']), display_labels = [False, True]).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "c7938d31-f04a-4bea-e2fa-80fd1c912d48",
        "id": "-NXuUaZFVf2i"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3f85a530a0>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-shot"
      ],
      "metadata": {
        "id": "BaQ4QGukVf2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_one_results = data_results[data_results['Setting']=='one_shot']"
      ],
      "metadata": {
        "id": "pmoAbo0TVf2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(gold_file_csv['Label'],balanced_one_results['Label']), display_labels = [False, True]).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "382bef89-8af4-4490-d21c-6b8cd522eaa3",
        "id": "YbXEOkaSVf2k"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3f858cab80>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html Ensemble.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeurYxlct-mF",
        "outputId": "b68b49ed-2534-4317-8ca6-e5a9e1a94e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Ensemble.ipynb to html\n",
            "[NbConvertApp] Writing 544616 bytes to Ensemble.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}